{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-1-4bc56731f3c8>:2 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-97c20c9652a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'local[*]'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rm -rf metastore_db/*.lck'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spark/lib/python3.6/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \"\"\"\n\u001b[1;32m    114\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_spark_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mCallSite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m~/anaconda3/envs/spark/lib/python3.6/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    297\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 299\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    300\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-1-4bc56731f3c8>:2 "
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext('local[*]')\n",
    "\n",
    "!rm -rf metastore_db/*.lck\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "sqlc = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1\n",
    "- Load the train and test sets\n",
    "- Check the schema, the variables have their right types?\n",
    "- If not, how to correctly load the datasets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
    "\n",
    "titanicSchemaTrain = StructType([StructField(\"PassengerId\", IntegerType(), True),\n",
    "                           StructField(\"Survived\", IntegerType(), True),\n",
    "                           StructField(\"Pclass\",  IntegerType(), True), \n",
    "                           StructField(\"Name\",  StringType(), True), \n",
    "                           StructField(\"Sex\",  StringType(), True), \n",
    "                           StructField(\"Age\",  IntegerType(), True), \n",
    "                           StructField(\"SibSp\",  IntegerType(), True), \n",
    "                           StructField(\"Parch\",  IntegerType(), True), \n",
    "                           StructField(\"Ticket\",  StringType(), True), \n",
    "                            StructField(\"Fare\",  FloatType(), True), \n",
    "                            StructField(\"Cabin\",  StringType(), True),\n",
    "                           StructField(\"Embarked\",  StringType(), True)]     \n",
    "                          )\n",
    "\n",
    "titanicSchemaTest = StructType([StructField(\"PassengerId\", IntegerType(), True),\n",
    "                           StructField(\"Pclass\",  IntegerType(), True), \n",
    "                           StructField(\"Name\",  StringType(), True), \n",
    "                           StructField(\"Sex\",  StringType(), True), \n",
    "                           StructField(\"Age\",  FloatType(), True), \n",
    "                           StructField(\"SibSp\",  IntegerType(), True), \n",
    "                           StructField(\"Parch\",  IntegerType(), True), \n",
    "                           StructField(\"Ticket\",  StringType(), True), \n",
    "                            StructField(\"Fare\",  FloatType(), True), \n",
    "                            StructField(\"Cabin\",  StringType(), True),\n",
    "                               StructField(\"Embarked\",  StringType(), True)]\n",
    "                          )\n",
    "df_train = sqlc.read.load(path=\"data/train.csv\", \n",
    "                          format=\"com.databricks.spark.csv\", \n",
    "                          schema=titanicSchemaTrain,\n",
    "                          header=True)\n",
    "\n",
    "df_test = sqlc.read.load(path=\"data/test.csv\", \n",
    "                          format=\"com.databricks.spark.csv\", \n",
    "                          schema=titanicSchemaTest, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2\n",
    "- Explore the features of your dataset\n",
    "- You can use DataFrame's ***describe*** method to get summary statistics\n",
    "    - hint: ***toPandas*** may be useful to ease the manipulation of small dataframes\n",
    "- Are there any ***NaN*** values in your dataset?\n",
    "- If so, define value/values to fill these ***NaN*** values\n",
    "    - hint: ***na*** property of DataFrames provide several methods of handling NA values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3\n",
    "- How to handle categorical features?\n",
    "    - hint: check the Estimators and Transformers\n",
    "- Assemble all desired features into a Vector using the VectorAssembler Transformer\n",
    "- Make sure to end up with a DataFrame with two columns: ***Survived*** and ***vFeatures***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_mean = df_train.describe().toPandas().set_index(\"summary\").loc['mean','Age']\n",
    "def remove_useless_features(df):\n",
    "    return df.drop(\"Cabin\")\n",
    "df_train = remove_useless_features(df_train)\n",
    "df_test = remove_useless_features(df_test)\n",
    "from numpy import NaN\n",
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import isnull, isnan, when, count, col\n",
    "\n",
    "def average_missing_features(df):\n",
    "    df = df.withColumn(\"age\", when(col('age').isNull(), age_mean).otherwise(col('age'))) \n",
    "    return df\n",
    "\n",
    "\n",
    "df_train =     average_missing_features(df_train)\n",
    "df_test  =     average_missing_features(df_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Step 3\n",
    "How to handle categorical features?\n",
    "hint: check the Estimators and Transformers\n",
    "Assemble all desired features into a Vector using the VectorAssembler Transformer\n",
    "Make sure to end up with a DataFrame with two columns: Survived and vFeatures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "def categorize_df(df):\n",
    "    indexerS = StringIndexer(inputCol=\"Sex\", outputCol=\"SexC\")\n",
    "    indexerE = StringIndexer(inputCol=\"Embarked\", outputCol=\"EmbarkedC\")\n",
    "    \n",
    "    df = indexerS.fit(df).transform(df)\n",
    "    df = indexerE.fit(df).transform(df)\n",
    "    \n",
    "    df = df.drop(\"Sex\", \"Embarked\")\n",
    "    return df\n",
    "\n",
    "df_train = categorize_df(df_train)\n",
    "df_test = categorize_df(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
    "def convert_age(df):\n",
    "    df = df.withColumn(\"age\", col(\"age\").cast(FloatType()))\n",
    "    return df\n",
    "df_train = convert_age(df_train)\n",
    "df_test = convert_age(df_test)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "def vectorize(df):\n",
    "\n",
    "    assembler = VectorAssembler(inputCols = \\\n",
    "                [\"Pclass\",\"age\", \"SibSp\", \"Parch\", \"Fare\", \"SexC\", \"EmbarkedC\"], outputCol = \"vFeatures\")\n",
    "\n",
    "    df = assembler.transform(df)\n",
    "    df = df['PassengerId','vFeatures','Survived']\n",
    "    return df\n",
    "\n",
    "df_train = vectorize(df_train)\n",
    "#df_test = vectorize(df_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "### INSERT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4\n",
    "- In Step 5, you will apply a normalization Estimator\n",
    "- BUT, it does not accept feature vectors of the Sparse type\n",
    "- So, it is neccessary to apply an User Defined Function to make all features vectors of type VectorUDT\n",
    "- In this step, you only have to replace ***YOUR DATAFRAME*** and ***NEW DATAFRAME*** with your variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "from pyspark.ml.linalg import VectorUDT, Vectors\n",
    "\n",
    "to_vec = UserDefinedFunction(lambda x: Vectors.dense(x.toArray()), VectorUDT())\n",
    "\n",
    "<NEW DATAFRAME> = <YOUR DATAFRAME>.select(\"Survived\", to_vec(\"vFeatures\").alias(\"features\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5\n",
    "- Apply a normalization Estimator of your choice to the ***features*** vector obtained in Step 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "### INSERT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6\n",
    "- Train a classifier of your choice (for instance, Random Forest) using your dataset of LabeledPoints\n",
    "- Make predictions for the training data\n",
    "- Use the Binary Classification Evaluator to evaluate your model on the training data\n",
    "- How is your model performing? Try to tune its parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import RandomForestClassificationModel\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "### INSERT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7\n",
    "- Take a look at the test data - use DataFrame's ***createOrReplaceTempView*** method to perform SQL queries over the data\n",
    "    - hint: check if there are any NULL values in the dataset - if so, handle them\n",
    "- Apply the transformations to the test data\n",
    "    - hint: you can use Pipelines to chain several Estimators/Transformers\n",
    "    - warning: unfortunately, it is not possible to include the UDF from Step 4 in the Pipeline\n",
    "- Make predictions using the model previously trained and the transformed test data\n",
    "- Save it as ***submission.csv*** and submit it to Kaggle\n",
    "- What was your score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "### INSERT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_predictions = <YOUR PREDICTIONS DATAFRAME>.select(\"prediction\").toPandas().reset_index()\n",
    "df_predictions['index'] = df_predictions['index'] + 892\n",
    "df_predictions.columns = ['PassengerId', 'Survived']\n",
    "\n",
    "df_predictions.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result = ???%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
