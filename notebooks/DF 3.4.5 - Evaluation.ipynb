{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext('local[*]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!rm -rf metastore_db/*.lck\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "sqlc = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "data = MLUtils.loadLibSVMFile(sc, \"sample_libsvm_data.txt\").toDF()\n",
    "data = MLUtils.convertVectorColumnsToML(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|(692,[127,128,129...|  0.0|\n",
      "|(692,[158,159,160...|  1.0|\n",
      "|(692,[124,125,126...|  1.0|\n",
      "|(692,[152,153,154...|  1.0|\n",
      "|(692,[151,152,153...|  1.0|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: (692,[235,243,244,271,272,273,300,301,323,328,350,351,356,372,373,378,379,385,400,405,406,407,413,427,428,433,434,435,455,456,461,462,483,484,489,490,496,511,512,517,539,540,568],[-9.92459585713e-05,-7.44271434606e-05,-0.00014946668662,-8.6967482907e-06,-0.000333320469687,-1.92548634575e-05,-0.000374657909184,-5.83975274944e-05,1.85147640777e-05,-0.000169305742381,4.08441117653e-05,9.64558496765e-05,-4.68086020898e-05,-8.783272557e-05,-3.61104292768e-05,0.000170455837293,0.000114090039273,-4.36014282826e-05,-8.45178184506e-05,8.85236144315e-05,0.000806253528489,0.000306768274123,-7.80793924807e-05,-5.83311217755e-06,-7.96741259275e-05,0.000293854701602,0.000921747131981,0.000120244897667,-0.000152916771459,-7.85612401905e-05,0.000168090224153,0.000879306698266,-0.000410431307045,-4.55008151983e-05,0.000159085881894,0.000189891435973,-9.17662279933e-05,-0.000490257348971,-0.00032795102547,0.000169783642593,-0.0007204108039,-0.000430967786898,-0.000331023310072]) Intercept: 0.19211742783934294\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import BinaryLogisticRegressionSummary\n",
    "\n",
    "trainingData, testData = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "logr = LogisticRegression().setMaxIter(10).setRegParam(0.3).setElasticNetParam(0.8)\n",
    "\n",
    "logrModel = logr.fit(trainingData)\n",
    "\n",
    "print(\"Weights: %s Intercept: %s\" % (logrModel.coefficients, logrModel.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "predictionsLogR = logrModel.transform(testData)\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator().setLabelCol(\"label\") \\\n",
    "                            .setRawPredictionCol(\"rawPrediction\") \\\n",
    "                            .setMetricName(\"areaUnderROC\")\n",
    "\n",
    "roc = evaluator.evaluate(predictionsLogR)\n",
    "print(roc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, IndexToString, VectorIndexer\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import RandomForestClassificationModel\n",
    "\n",
    "labelIndexer = StringIndexer().setInputCol(\"label\") \\\n",
    "                                .setOutputCol(\"indexedLabel\").fit(data)\n",
    "\n",
    "labelConverter = IndexToString().setInputCol(\"prediction\") \\\n",
    "                                .setOutputCol(\"predictedLabel\") \\\n",
    "                                .setLabels(labelIndexer.labels)\n",
    "\n",
    "featureIndexer = VectorIndexer().setInputCol(\"features\") \\\n",
    "                                .setOutputCol(\"indexedFeatures\") \\\n",
    "                                .setMaxCategories(4).fit(data)\n",
    "\n",
    "rfC = RandomForestClassifier().setLabelCol(\"indexedLabel\") \\\n",
    "                                .setFeaturesCol(\"indexedFeatures\") \\\n",
    "                                .setNumTrees(3)\n",
    "        \n",
    "pipelineRFC = Pipeline().setStages([labelIndexer, featureIndexer, rfC, labelConverter])\n",
    "\n",
    "modelRFC = pipelineRFC.fit(trainingData)\n",
    "\n",
    "predictionsRFC = modelRFC.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator().setLabelCol(\"indexedLabel\") \\\n",
    "                                        .setPredictionCol(\"prediction\") \\\n",
    "                                        .setMetricName(\"accuracy\")\n",
    "\n",
    "accuracy = evaluator.evaluate(predictionsRFC)\n",
    "\n",
    "print(\"Test Error = %s\" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.regression import RandomForestRegressionModel\n",
    "\n",
    "rfR = RandomForestRegressor().setLabelCol(\"label\").setFeaturesCol(\"indexedFeatures\")\n",
    "\n",
    "pipelineRFR = Pipeline().setStages([featureIndexer, rfR])\n",
    "\n",
    "modelRFR = pipelineRFR.fit(trainingData)\n",
    "\n",
    "predictionsRFR = modelRFR.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) = 0.10648608225625411\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator = RegressionEvaluator().setLabelCol(\"label\") \\\n",
    "                                .setPredictionCol(\"prediction\") \\\n",
    "                                .setMetricName(\"rmse\")\n",
    "\n",
    "rmse = evaluator.evaluate(predictionsRFR)\n",
    "\n",
    "print(\"Root Mean Squared Error (RMSE) = %s\" % rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: (692,[235,243,244,271,272,273,300,301,323,328,350,351,356,372,373,378,379,385,400,405,406,407,413,427,428,433,434,435,455,456,461,462,483,484,489,490,496,511,512,517,539,540,568],[-9.92459585713e-05,-7.44271434606e-05,-0.00014946668662,-8.6967482907e-06,-0.000333320469687,-1.92548634575e-05,-0.000374657909184,-5.83975274944e-05,1.85147640777e-05,-0.000169305742381,4.08441117653e-05,9.64558496765e-05,-4.68086020898e-05,-8.783272557e-05,-3.61104292768e-05,0.000170455837293,0.000114090039273,-4.36014282826e-05,-8.45178184506e-05,8.85236144315e-05,0.000806253528489,0.000306768274123,-7.80793924807e-05,-5.83311217755e-06,-7.96741259275e-05,0.000293854701602,0.000921747131981,0.000120244897667,-0.000152916771459,-7.85612401905e-05,0.000168090224153,0.000879306698266,-0.000410431307045,-4.55008151983e-05,0.000159085881894,0.000189891435973,-9.17662279933e-05,-0.000490257348971,-0.00032795102547,0.000169783642593,-0.0007204108039,-0.000430967786898,-0.000331023310072]) Intercept: 0.19211742783934294\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "logr = LogisticRegression().setMaxIter(10).setRegParam(0.3).setElasticNetParam(0.8)\n",
    "\n",
    "logrModel = logr.fit(trainingData)\n",
    "\n",
    "print(\"Weights: %s Intercept: %s\" % (logrModel.coefficients, logrModel.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingSummaryLR = logrModel.summary\n",
    "trainingSummaryLR.areaUnderROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------+\n",
      "|         threshold|          F-Measure|\n",
      "+------------------+-------------------+\n",
      "|0.7923937273050735|0.04878048780487806|\n",
      "|0.7916716001125358|0.09523809523809523|\n",
      "|0.7906437230222552|0.13953488372093023|\n",
      "+------------------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fMeasure = trainingSummaryLR.fMeasureByThreshold\n",
    "\n",
    "fMeasure.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "0.5653094538250638\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "maxFMeasure = fMeasure.agg({\"F-Measure\": \"max\"}).head()[0]\n",
    "print(maxFMeasure)\n",
    "maxFMeasure = fMeasure.agg(F.max(F.col(\"F-Measure\"))).head()[0]\n",
    "print(maxFMeasure)\n",
    "\n",
    "bestThreshold = fMeasure.where(F.col(\"F-Measure\") == maxFMeasure).select(\"threshold\").head()[0]\n",
    "print(bestThreshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+\n",
      "|recall|precision|\n",
      "+------+---------+\n",
      "|   0.0|      1.0|\n",
      "| 0.025|      1.0|\n",
      "|  0.05|      1.0|\n",
      "+------+---------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+------------------+---------+\n",
      "|         threshold|precision|\n",
      "+------------------+---------+\n",
      "|0.7923937273050735|      1.0|\n",
      "|0.7916716001125358|      1.0|\n",
      "|0.7906437230222552|      1.0|\n",
      "+------------------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainingSummaryLR.pr.show(3)\n",
    "trainingSummaryLR.precisionByThreshold.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------+\n",
      "|         threshold|recall|\n",
      "+------------------+------+\n",
      "|0.7923937273050735| 0.025|\n",
      "|0.7916716001125358|  0.05|\n",
      "|0.7906437230222552| 0.075|\n",
      "+------------------+------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+---+-----+\n",
      "|FPR|  TPR|\n",
      "+---+-----+\n",
      "|0.0|  0.0|\n",
      "|0.0|0.025|\n",
      "|0.0| 0.05|\n",
      "+---+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainingSummaryLR.recallByThreshold.show(3)\n",
    "trainingSummaryLR.roc.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-4.0919617776e-06,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-4.42660730291e-06,-1.65512647194e-05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-2.70110897902e-06,-4.8629773194e-05,-5.5958141591e-06,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-5.66682667167e-05,-5.9878205346e-06,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,2.41437590075e-05,0.0,0.0,0.0,0.0,-1.99749285826e-05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,5.29715034096e-05,7.07855742231e-05,0.0,0.0,0.0,0.0,-5.11031754813e-06,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-7.24693434693e-06,-6.03432275329e-06,0.0,0.0,0.0,-0.0,9.65540243175e-05,8.1951158951e-05,0.0,0.0,0.0,0.0,0.0,-6.22218903781e-06,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-7.2098719408e-06,-4.41633640672e-06,0.0,0.0,0.0,6.82967113921e-05,0.000153405101074,0.000110248299103,0.0,0.0,0.0,0.0,0.0,-6.37567545593e-06,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-6.11918507881e-06,-6.7452265343e-06,0.0,0.0,0.0,0.0,0.000105831747435,0.000185458317783,8.63443804463e-05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-1.93656849558e-05,-6.302854703e-06,0.0,0.0,0.0,0.0,9.3574945474e-05,0.000180187685847,3.42532450011e-06,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-6.35425702562e-05,-4.6224533981e-06,0.0,0.0,0.0,0.0,9.03185651903e-05,0.000104055420396,0.0,0.0,0.0,0.0,0.0,-7.39790583383e-06,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-7.01530055904e-05,-4.95910694574e-05,0.0,0.0,0.0,0.0,9.68448236694e-05,1.08631297539e-06,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-7.26386217556e-05,-6.69325940443e-05,0.0,0.0,0.0,0.0,2.21363970344e-06,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-2.61219455418e-05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0] Intercept: 0.40737357275754904\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression().setMaxIter(10).setRegParam(0.3).setElasticNetParam(0.8)\n",
    "\n",
    "lrModel = lr.fit(trainingData)\n",
    "\n",
    "print(\"Weights: %s Intercept: %s\" % (lrModel.coefficients, lrModel.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05511378820975742\n",
      "0.2669094074506451\n",
      "0.07510961541348907\n",
      "0.6958060575753693\n"
     ]
    }
   ],
   "source": [
    "trainingSummaryLLS = lrModel.summary\n",
    "\n",
    "print(trainingSummaryLLS.explainedVariance)\n",
    "\n",
    "print(trainingSummaryLLS.meanAbsoluteError)\n",
    "\n",
    "print(trainingSummaryLLS.meanSquaredError)\n",
    "\n",
    "print(trainingSummaryLLS.r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           residuals|\n",
      "+--------------------+\n",
      "|-0.27172696247114375|\n",
      "|    0.50242170147003|\n",
      "| 0.29858531755853046|\n",
      "+--------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "0.2740613351304577\n"
     ]
    }
   ],
   "source": [
    "trainingSummaryLLS.residuals.show(3)\n",
    "\n",
    "print(trainingSummaryLLS.rootMeanSquaredError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
