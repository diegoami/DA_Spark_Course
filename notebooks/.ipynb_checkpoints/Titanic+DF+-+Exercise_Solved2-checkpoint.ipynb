{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext('local[*]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!rm -rf metastore_db/*.lck\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "sqlc = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1\n",
    "- Load the train and test sets\n",
    "- Check the schema, the variables have their right types?\n",
    "- If not, how to correctly load the datasets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "\n",
    "customSchema = StructType([StructField(\"PassengerId\", IntegerType(), True),\n",
    "                           StructField(\"Survived\", DoubleType(), True),\n",
    "                           StructField(\"Pclass\", IntegerType(), True), \n",
    "                           StructField(\"Name\", StringType(), True),\n",
    "                           StructField(\"Sex\", StringType(), True),\n",
    "                           StructField(\"Age\", DoubleType(), True),\n",
    "                           StructField(\"SibSp\", IntegerType(), True),\n",
    "                           StructField(\"Parch\", IntegerType(), True),\n",
    "                           StructField(\"Ticket\", StringType(), True),\n",
    "                           StructField(\"Fare\", DoubleType(), True),\n",
    "                           StructField(\"Cabin\", StringType(), True),\n",
    "                           StructField(\"Embarked\", StringType(), True)])\n",
    "\n",
    "customSchema2 = StructType([StructField(\"PassengerId\", IntegerType(), True),\n",
    "                           StructField(\"Pclass\", IntegerType(), True), \n",
    "                           StructField(\"Name\", StringType(), True),\n",
    "                           StructField(\"Sex\", StringType(), True),\n",
    "                           StructField(\"Age\", DoubleType(), True),\n",
    "                           StructField(\"SibSp\", IntegerType(), True),\n",
    "                           StructField(\"Parch\", IntegerType(), True),\n",
    "                           StructField(\"Ticket\", StringType(), True),\n",
    "                           StructField(\"Fare\", DoubleType(), True),\n",
    "                           StructField(\"Cabin\", StringType(), True),\n",
    "                           StructField(\"Embarked\", StringType(), True)])\n",
    "\n",
    "train = sqlc.read.csv(\"train.csv\", header=True, schema=customSchema)\n",
    "test = sqlc.read.csv(\"test.csv\", header=True, schema=customSchema2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2\n",
    "- Explore the features of your dataset\n",
    "- You can use DataFrame's ***describe*** method to get summary statistics\n",
    "    - hint: ***toPandas*** may be useful to ease the manipulation of small dataframes\n",
    "- Are there any ***NaN*** values in your dataset?\n",
    "- If so, define value/values to fill these ***NaN*** values\n",
    "    - hint: ***na*** property of DataFrames provide several methods of handling NA values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>summary</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>891</td>\n",
       "      <td>891</td>\n",
       "      <td>891</td>\n",
       "      <td>891</td>\n",
       "      <td>891</td>\n",
       "      <td>714</td>\n",
       "      <td>891</td>\n",
       "      <td>891</td>\n",
       "      <td>891</td>\n",
       "      <td>891</td>\n",
       "      <td>204</td>\n",
       "      <td>889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>446.0</td>\n",
       "      <td>0.3838383838383838</td>\n",
       "      <td>2.308641975308642</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>29.69911764705882</td>\n",
       "      <td>0.5230078563411896</td>\n",
       "      <td>0.38159371492704824</td>\n",
       "      <td>260318.54916792738</td>\n",
       "      <td>32.2042079685746</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stddev</th>\n",
       "      <td>257.3538420152301</td>\n",
       "      <td>0.48659245426485753</td>\n",
       "      <td>0.8360712409770491</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>14.526497332334035</td>\n",
       "      <td>1.1027434322934315</td>\n",
       "      <td>0.8060572211299488</td>\n",
       "      <td>471609.26868834975</td>\n",
       "      <td>49.69342859718089</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Andersson, Mr. August Edvard (\"\"Wennerstrom\"\")\"</td>\n",
       "      <td>female</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>110152</td>\n",
       "      <td>0.0</td>\n",
       "      <td>A10</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>891</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>van Melkebeke, Mr. Philemon</td>\n",
       "      <td>male</td>\n",
       "      <td>80.0</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>WE/P 5735</td>\n",
       "      <td>512.3292</td>\n",
       "      <td>T</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               PassengerId             Survived              Pclass  \\\n",
       "summary                                                               \n",
       "count                  891                  891                 891   \n",
       "mean                 446.0   0.3838383838383838   2.308641975308642   \n",
       "stddev   257.3538420152301  0.48659245426485753  0.8360712409770491   \n",
       "min                      1                  0.0                   1   \n",
       "max                    891                  1.0                   3   \n",
       "\n",
       "                                                     Name     Sex  \\\n",
       "summary                                                             \n",
       "count                                                 891     891   \n",
       "mean                                                 None    None   \n",
       "stddev                                               None    None   \n",
       "min      \"Andersson, Mr. August Edvard (\"\"Wennerstrom\"\")\"  female   \n",
       "max                           van Melkebeke, Mr. Philemon    male   \n",
       "\n",
       "                        Age               SibSp                Parch  \\\n",
       "summary                                                                \n",
       "count                   714                 891                  891   \n",
       "mean      29.69911764705882  0.5230078563411896  0.38159371492704824   \n",
       "stddev   14.526497332334035  1.1027434322934315   0.8060572211299488   \n",
       "min                    0.42                   0                    0   \n",
       "max                    80.0                   8                    6   \n",
       "\n",
       "                     Ticket               Fare Cabin Embarked  \n",
       "summary                                                        \n",
       "count                   891                891   204      889  \n",
       "mean     260318.54916792738   32.2042079685746  None     None  \n",
       "stddev   471609.26868834975  49.69342859718089  None     None  \n",
       "min                  110152                0.0   A10        C  \n",
       "max               WE/P 5735           512.3292     T        S  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating summary statistics and turning it into Pandas DF\n",
    "train_desc = train.describe().toPandas().set_index('summary')\n",
    "train_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Fare': 0.2573065223849626, 'Age': 0.010539215871285682, 'Pclass': -0.3384810359610151, 'Parch': 0.08162940708348339, 'SibSp': -0.0353224988857356}\n",
      "{'SibSp': 0, 'Embarked': 2, 'Fare': 0, 'Ticket': 0, 'Cabin': 687, 'Name': 0, 'Age': 177, 'Pclass': 0, 'Sex': 0, 'Survived': 0, 'PassengerId': 0, 'Parch': 0}\n",
      "29.69911764705882\n"
     ]
    }
   ],
   "source": [
    "# Computing correlations between Survived and some features\n",
    "print({col:train.stat.corr('Survived',col) for col in ['Pclass','Age','SibSp','Parch','Fare']})\n",
    "\n",
    "# Checking which columns have NULL values\n",
    "print({col:train.where(train[col].isNull()).count() for col in train.columns})\n",
    "\n",
    "# Taking the mean age from the Pandas DF\n",
    "ageMean = float(train_desc.loc['mean']['Age'])\n",
    "print(ageMean)\n",
    "\n",
    "# Filling the Age in both train and test datasets\n",
    "trainFilled = train.na.fill({'Age': ageMean, 'Embarked': 'S'})\n",
    "testFilled = test.na.fill({'Age': ageMean, 'Embarked': 'S'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------------------+\n",
      "|   Sex|PClass|          avg(age)|\n",
      "+------+------+------------------+\n",
      "|  male|     3|26.507588932806325|\n",
      "|female|     3|             21.75|\n",
      "|female|     1| 34.61176470588235|\n",
      "|female|     2|28.722972972972972|\n",
      "|  male|     2| 30.74070707070707|\n",
      "|  male|     1| 41.28138613861386|\n",
      "+------+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "train.groupby('Sex','PClass').agg(F.mean('age')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3\n",
    "- How to handle categorical features?\n",
    "    - hint: check the Estimators and Transformers\n",
    "- Assemble all desired features into a Vector using the VectorAssembler Transformer\n",
    "- Make sure to end up with a DataFrame with two columns: ***Survived*** and ***vFeatures***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "# Applying Estimators and Transformators\n",
    "# Here, I actually fitted and transformed them on the training data\n",
    "# with the purpose of being able to check the intermediate steps\n",
    "\n",
    "indexer1 = (StringIndexer()\n",
    "           .setInputCol(\"Embarked\")\n",
    "           .setOutputCol(\"nEmbarked\")\n",
    "           .setHandleInvalid('skip'))\n",
    "\n",
    "indexed1 = indexer1.fit(trainFilled).transform(trainFilled)\n",
    "\n",
    "indexer2 = (StringIndexer()\n",
    "           .setInputCol(\"Sex\")\n",
    "           .setOutputCol(\"nSex\")\n",
    "           .setHandleInvalid('skip'))\n",
    "\n",
    "indexed2 = indexer2.fit(indexed1).transform(indexed1)\n",
    "\n",
    "encoder1 = OneHotEncoder().setInputCol(\"nEmbarked\").setOutputCol(\"vEmbarked\")\n",
    "encoded1 = encoder1.transform(indexed2)\n",
    "\n",
    "encoder2 = OneHotEncoder().setInputCol(\"nSex\").setOutputCol(\"vSex\")\n",
    "encoded2 = encoder2.transform(encoded1)\n",
    "\n",
    "# Using a VectorAssembler to put together all feature columns\n",
    "assembler = VectorAssembler(inputCols=['Pclass',\n",
    "                                       'Age',\n",
    "                                       'SibSp',\n",
    "                                       'Parch',\n",
    "                                       'Fare',\n",
    "                                       'vSex',\n",
    "                                       'vEmbarked'], \n",
    "                            outputCol='vFeatures')\n",
    "\n",
    "assembled = assembler.transform(encoded2)\n",
    "\n",
    "# Keeping only the features and label columns to \n",
    "assembled2 = assembled.select(\"Survived\",\"vFeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|Survived|           vFeatures|\n",
      "+--------+--------------------+\n",
      "|     0.0|[3.0,22.0,1.0,0.0...|\n",
      "|     1.0|[1.0,38.0,1.0,0.0...|\n",
      "|     1.0|(8,[0,1,4,6],[3.0...|\n",
      "|     1.0|[1.0,35.0,1.0,0.0...|\n",
      "|     0.0|[3.0,35.0,0.0,0.0...|\n",
      "|     0.0|(8,[0,1,4,5],[3.0...|\n",
      "|     0.0|[1.0,54.0,0.0,0.0...|\n",
      "|     0.0|[3.0,2.0,3.0,1.0,...|\n",
      "|     1.0|[3.0,27.0,0.0,2.0...|\n",
      "|     1.0|[2.0,14.0,1.0,0.0...|\n",
      "+--------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assembled2.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4\n",
    "- In Step 5, you will apply a normalization Estimator\n",
    "- BUT, it does not accept feature vectors of the Sparse type\n",
    "- So, it is neccessary to apply an User Defined Function to make all features vectors of type VectorUDT\n",
    "- In this step, you only have to replace ***YOUR DATAFRAME*** and ***NEW DATAFRAME*** with your variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "from pyspark.ml.linalg import VectorUDT, Vectors\n",
    "\n",
    "to_vec = UserDefinedFunction(lambda x: Vectors.dense(x.toArray()), \n",
    "                             VectorUDT())\n",
    "\n",
    "# NOT NEEDED ANYMORE, SPARK DOES THE CONVERSION AUTOMATICALLY!\n",
    "assembled3 = assembled2.select(\"Survived\", to_vec(\"vFeatures\").alias(\"features\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5\n",
    "- Apply a normalization Estimator of your choice to the ***features*** vector obtained in Step 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "scaler = StandardScaler().setInputCol(\"vFeatures\").setOutputCol(\"scaledFeat\").setWithStd(True).setWithMean(True)\n",
    "scalerModel = scaler.fit(assembled2)\n",
    "scaled = scalerModel.transform(assembled2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+\n",
      "|Survived|           vFeatures|          scaledFeat|\n",
      "+--------+--------------------+--------------------+\n",
      "|     0.0|[3.0,22.0,1.0,0.0...|[0.82691281652436...|\n",
      "|     1.0|[1.0,38.0,1.0,0.0...|[-1.5652278312782...|\n",
      "|     1.0|(8,[0,1,4,6],[3.0...|[0.82691281652436...|\n",
      "|     1.0|[1.0,35.0,1.0,0.0...|[-1.5652278312782...|\n",
      "|     0.0|[3.0,35.0,0.0,0.0...|[0.82691281652436...|\n",
      "|     0.0|(8,[0,1,4,5],[3.0...|[0.82691281652436...|\n",
      "|     0.0|[1.0,54.0,0.0,0.0...|[-1.5652278312782...|\n",
      "|     0.0|[3.0,2.0,3.0,1.0,...|[0.82691281652436...|\n",
      "|     1.0|[3.0,27.0,0.0,2.0...|[0.82691281652436...|\n",
      "|     1.0|[2.0,14.0,1.0,0.0...|[-0.3691575073769...|\n",
      "+--------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scaled.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages=[indexer1,\n",
    "                            indexer2,\n",
    "                            encoder1, \n",
    "                            encoder2, \n",
    "                            assembler,\n",
    "                            scaler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = pipeline.fit(trainFilled)\n",
    "scaled = model.transform(trainFilled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|Survived|          scaledFeat|\n",
      "+--------+--------------------+\n",
      "|     0.0|[0.82691281652436...|\n",
      "|     1.0|[-1.5652278312782...|\n",
      "|     1.0|[0.82691281652436...|\n",
      "|     1.0|[-1.5652278312782...|\n",
      "|     0.0|[0.82691281652436...|\n",
      "|     0.0|[0.82691281652436...|\n",
      "|     0.0|[-1.5652278312782...|\n",
      "|     0.0|[0.82691281652436...|\n",
      "|     1.0|[0.82691281652436...|\n",
      "|     1.0|[-0.3691575073769...|\n",
      "+--------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scaled.select('Survived', 'scaledFeat').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6\n",
    "- Train a classifier of your choice (for instance, Random Forest) using your dataset of LabeledPoints\n",
    "- Make predictions for the training data\n",
    "- Use the Binary Classification Evaluator to evaluate your model on the training data\n",
    "- How is your model performing? Try to tune its parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import RandomForestClassificationModel\n",
    "\n",
    "# Trains a RF classifier and make predictions\n",
    "rfC = RandomForestClassifier().setLabelCol(\"Survived\") \\\n",
    "                                .setFeaturesCol(\"scaledFeat\") \\\n",
    "                                .setNumTrees(50)\n",
    "\n",
    "model = rfC.fit(scaled)\n",
    "\n",
    "predictions = model.transform(scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9082036451176516\n",
      "0.8529741863075196\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "# Defines an evaluator based on the metric areaUnderROC\n",
    "evaluator = BinaryClassificationEvaluator().setLabelCol(\"Survived\") \\\n",
    "                            .setRawPredictionCol(\"rawPrediction\") \\\n",
    "                            .setMetricName(\"areaUnderROC\")\n",
    "\n",
    "# Evaluate the predictions\n",
    "roc = evaluator.evaluate(predictions)\n",
    "\n",
    "print(roc)\n",
    "\n",
    "ev2 = (MulticlassClassificationEvaluator()\n",
    "       .setLabelCol('Survived')\n",
    "       .setPredictionCol('prediction')\n",
    "       .setMetricName('accuracy'))\n",
    "\n",
    "acc = ev2.evaluate(predictions)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7\n",
    "- Take a look at the test data - use DataFrame's ***createOrReplaceTempView*** method to perform SQL queries over the data\n",
    "    - hint: check if there are any NULL values in the dataset - if so, handle them\n",
    "- Apply the transformations to the test data\n",
    "    - hint: you can use Pipelines to chain several Estimators/Transformers\n",
    "    - warning: unfortunately, it is not possible to include the UDF from Step 4 in the Pipeline\n",
    "- Make predictions using the model previously trained and the transformed test data\n",
    "- Save it as ***submission.csv*** and submit it to Kaggle\n",
    "- What was your score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Age': 0, 'Pclass': 0, 'Cabin': 327, 'Sex': 0, 'Embarked': 0, 'SibSp': 0, 'Fare': 1, 'Parch': 0, 'PassengerId': 0, 'Ticket': 0, 'Name': 0}\n",
      "13.675550101832997\n"
     ]
    }
   ],
   "source": [
    "# Make the test set a \"table\"\n",
    "testFilled.createOrReplaceTempView('test')\n",
    "\n",
    "# Runs a series of SQL queries to get the number of null values in the test set\n",
    "print({col: sqlc.sql(\"select * from test where \" + col + \" is null\").count() for col in testFilled.columns})\n",
    "\n",
    "# So, there is one null Fare, let's check it\n",
    "sqlc.sql(\"select * from test where Fare is null\").toPandas()\n",
    "\n",
    "# Since the Fare is highly dependent on the class, it makes more sense to use the average for the given class\n",
    "# But we need to take the average from the TRAINING set\n",
    "trainFilled.createOrReplaceTempView('train')\n",
    "avgFare = sqlc.sql(\"select mean(Fare) from train where Pclass = 3\").take(1)[0][0]\n",
    "print(avgFare)\n",
    "\n",
    "# Fill the missing value with the calculated average\n",
    "testFilled = testFilled.na.fill({'Fare': avgFare})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages=[indexer1,\n",
    "                            indexer2,\n",
    "                            encoder1, \n",
    "                            encoder2, \n",
    "                            assembler,\n",
    "                            scaler,\n",
    "                            rfC])\n",
    "\n",
    "model = pipeline.fit(trainFilled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = model.transform(testFilled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions = predictions.select('PassengerId',F.col('prediction').alias('Survived')).toPandas()\n",
    "df_predictions.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "answers = sqlc.read.csv('titanic_answers.csv', header=True)\n",
    "answers = answers.select('PassengerId',F.col('Survived').cast('Double'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_answer = predictions.join(answers, on='PassengerId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8095788704965916\n"
     ]
    }
   ],
   "source": [
    "roc = evaluator.evaluate(pred_answer)\n",
    "print(roc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.777511961722488\n"
     ]
    }
   ],
   "source": [
    "acc = ev2.evaluate(pred_answer)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Window Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------------------+\n",
      "|PClass|   Fare|           avgFare|\n",
      "+------+-------+------------------+\n",
      "|     1|    0.0|               0.0|\n",
      "|     1|    0.0|               0.0|\n",
      "|     1|    0.0|               0.0|\n",
      "|     1|    0.0|               0.0|\n",
      "|     1|    0.0|               0.0|\n",
      "|     1|    5.0|1.6666666666666667|\n",
      "|     1|25.5875|10.195833333333333|\n",
      "|     1| 25.925|18.837500000000002|\n",
      "|     1|25.9292|           25.8139|\n",
      "|     1|25.9292|           25.9278|\n",
      "|     1|   26.0|           25.9528|\n",
      "|     1|   26.0|           25.9764|\n",
      "|     1|26.2833| 26.09443333333333|\n",
      "|     1|26.2875|26.190266666666663|\n",
      "|     1|26.2875|26.286100000000005|\n",
      "|     1|26.2875|26.287500000000005|\n",
      "|     1|26.3875|26.320833333333336|\n",
      "|     1|  26.55| 26.40833333333333|\n",
      "|     1|  26.55|26.495833333333334|\n",
      "|     1|  26.55|             26.55|\n",
      "|     1|  26.55|             26.55|\n",
      "|     1|  26.55|             26.55|\n",
      "|     1|  26.55|             26.55|\n",
      "|     1|  26.55|             26.55|\n",
      "|     1|  26.55|             26.55|\n",
      "|     1|  26.55|             26.55|\n",
      "|     1|  26.55|             26.55|\n",
      "|     1|  26.55|             26.55|\n",
      "|     1|  26.55|             26.55|\n",
      "|     1|  26.55|             26.55|\n",
      "|     1|  26.55|             26.55|\n",
      "|     1|  26.55|             26.55|\n",
      "|     1|27.7208| 26.94026666666667|\n",
      "|     1|27.7208|27.330533333333335|\n",
      "|     1|27.7208|           27.7208|\n",
      "|     1|27.7208|           27.7208|\n",
      "|     1|  27.75| 27.73053333333333|\n",
      "|     1|   28.5|27.990266666666667|\n",
      "|     1|28.7125|28.320833333333336|\n",
      "|     1|   29.7| 28.97083333333333|\n",
      "|     1|   29.7|29.370833333333334|\n",
      "|     1|   29.7|              29.7|\n",
      "|     1|   30.0|              29.8|\n",
      "|     1|   30.0|29.900000000000002|\n",
      "|     1|   30.0|              30.0|\n",
      "|     1|   30.0|              30.0|\n",
      "|     1|   30.0|              30.0|\n",
      "|     1|   30.5|30.166666666666668|\n",
      "|     1|   30.5|30.333333333333332|\n",
      "|     1|   30.5|              30.5|\n",
      "+------+-------+------------------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Window\n",
    "w = (Window()\n",
    "     .partitionBy('PClass')\n",
    "     .orderBy('Fare')\n",
    "     .rowsBetween(Window.currentRow - 2, Window.currentRow))\n",
    "x = trainFilled.withColumn('avgFare', F.mean('Fare').over(w))\n",
    "x.select('PClass','Fare','avgFare').show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
